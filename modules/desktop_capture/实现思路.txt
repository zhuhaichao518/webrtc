实现思路：
\webrtc\src\modules\desktop_capture\win\screen_capturer_win_directx.cc
  ScreenCaptureFrameQueue<DxgiFrame>& frames =
      frame_queue_map_[current_screen_id_];
我们用一个DxgiFrameHW继承一下

startcapture之前设置是否以硬件编码 否则应该不生效 不然动态改动不好做。

DxgiFrame2应该会提前创建好texture2d 我希望实现在prepare函数里面
  if (!frame->Prepare(SelectedDesktopSize(monitor_id), monitor_id)) {
    return Result::FRAME_PREPARE_FAILED;
  }
更新:不需要prepare texture2d 让duplicator自己传递维护的device并且设置texture进来(如果是null的话)。

我们不应该使用texture_staging创建的texture 而是直接copy到target的texture里面
调用share的时候应当创建一个

std::atomic<bool> shared_bool;
DxgiFrame2应该有一个原子变量叫in_use 代表目前ffmpeg是否在处理这一帧
12个应该够用 sunshine里就是12个

  // Directly pass device and texture if it is in GPU.
  // Haichao: Why sometimes texture is nullptr?
  这里应该把frame的计数器+1
  //Todo:假如下一帧没变 不需要调用onframe
  if (frame->GetTexture() != nullptr){
    rtc::scoped_refptr<webrtc::NativeHandleBuffer> native_buffer = rtc::make_ref_counted<webrtc::NativeHandleBuffer>(frame->GetDevice(), frame->GetTexture(), w_ > 0 ? w_ : width,
    h_ > 0 ? h_ : height);
    OnFrame(webrtc::VideoFrame(native_buffer, 0, rtc::TimeMillis(),
                               webrtc::kVideoRotation_0));
    return;
  }


TODO:
refactor这样一个shared_ptr (scopedref):

加在VideoFrame里面,替换之前的GetDevice接口(析构的时候记得delete它):
NativeImage * native_image_;
struct NativeImage
{
    std::atomic<bool> in_use;
    ID3D11Device* device;
    ID3D11Texture2D* texture;
    NativeImage():in_use(false),device(nullptr),texture(nullptr){
    }
    ~NativeImage(){
        if (device!=nullptr){
            device->Release();
        }
        if (texture!=nullptr){
            texture->Release();
        }
        in_use = false;
    }
}

SetDevice(ID3D11Device* device){
    if (native_image_->device == device) return;
    if (native_image_->device != null){
        native_image_->device->Release();
    }
    native_image_->device = device;
    device->Addref();
}

SetTexture(ID3D11Texture2D* texture){
    if (native_image_->texture == texture) return;
    if (native_image_->texture != null){
        native_image_->texture->Release();
    }
    texture->Addref();
    native_image_->texture = texture;
}

当从list选出这一帧的时侯初始化它的VideoFrame成员变量的 in_use应该置为true,
直到ffmpeg解码完毕才应该置为false.
capturer负责SetDevice SetTexture赋予device和texture():

解码的时候应该能拿到native_image_变量 如果不为null应该可以拿到一个in_use=true(到时候dcheck一下)的
指针


AVD3D11VADeviceContext 是 FFmpeg 与 DirectX 11 视频加速解码（如使用 VA-API 或 DXVA2）交互时使用的结构。它允许 FFmpeg 利用 DirectX 11 的硬件加速能力进行视频解码或其他视频处理任务，如硬件加速的视频编码或格式转换。这个结构体主要用于初始化和配置与 DirectX 11 硬件加速相关的 FFmpeg 功能。

使用场景
AVD3D11VADeviceContext 主要在需要进行硬件加速视频解码或编码的应用程序中使用。它通过配置 FFmpeg 的硬件解码上下文来启用 DirectX 11 硬件加速。这对于需要处理高分辨率视频流（如 4K 视频播放或实时视频编码）的应用程序特别有用，因为硬件加速可以显著减少 CPU 负担，提高性能和效率。

基本步骤
创建 D3D11 设备和设备上下文：首先，你需要使用 DirectX 11 API 创建一个 ID3D11Device 和 ID3D11DeviceContext。

配置 AVD3D11VADeviceContext：然后，创建一个 AVD3D11VADeviceContext 并将其与你创建的 D3D11 设备和设备上下文相关联。

初始化 FFmpeg 硬件上下文：使用 av_hwdevice_ctx_create() 或类似函数初始化 FFmpeg 硬件上下文，将其设置为使用 DirectX 11 设备进行硬件加速。

使用硬件加速解码或编码：配置好硬件上下文后，你就可以使用 FFmpeg 进行硬件加速的视频解码或编码了。

示例代码
以下是使用 AVD3D11VADeviceContext 进行硬件加速视频解码的简化示例：

cpp
Copy code
// 假设你已经有了 D3D11 设备和设备上下文
ID3D11Device* d3d11Device;
ID3D11DeviceContext* d3d11DeviceContext;

// 创建 FFmpeg 硬件设备上下文
AVBufferRef* hwDeviceCtx = nullptr;
int err = av_hwdevice_ctx_create(&hwDeviceCtx, AV_HWDEVICE_TYPE_D3D11VA, NULL, NULL, 0);
if (err < 0) {
    // 错误处理
}

// 获取 AVHWDeviceContext 并配置 AVD3D11VADeviceContext
AVHWDeviceContext* hwDeviceContext = (AVHWDeviceContext*)hwDeviceCtx->data;
AVD3D11VADeviceContext* d3d11vaDeviceCtx = (AVD3D11VADeviceContext*)hwDeviceContext->hwctx;

d3d11vaDeviceCtx->device = d3d11Device;
d3d11vaDeviceCtx->device_context = d3d11DeviceContext;

// 使用 hwDeviceCtx 进行硬件加速的解码或其他操作
请注意，这个示例非常简化，实际使用时还需要考虑如何正确管理 DirectX 和 FFmpeg 的资源，以及如何处理错误。在使用硬件加速时，还需确保视频编解码器支持通过 D3D11 进行加速。

文档和进一步阅读
FFmpeg 官方文档和代码示例是了解如何使用 AVD3D11VADeviceContext 的好资源。
DirectX 11 文档，了解如何管理 D3D11 设备和上下文。
相关社区论坛和 Stack Overflow 上的讨论，可以提供使用 FFmpeg 进行硬件加速视频处理的实际经验和问题解决策略。
由于硬件加速视频处理涉及到多个复杂的组件，建议深入研究相关文档和示例代码，以更好地理解整个过程。



User
// 使用 hwDeviceCtx 进行硬件加速的解码或其他操作 这段可以再帮我写一点嘛 这对我很重要
ChatGPT
为了使用 hwDeviceCtx 进行硬件加速的解码操作，你需要完成几个步骤来确保 FFmpeg 正确地使用了你初始化的 DirectX 11 设备。以下是继续上述示例的进一步说明和代码示例，它将展示如何使用这个硬件上下文来进行视频流的硬件加速解码。

步骤 1: 打开解码器
首先，你需要找到合适的解码器并打开它。通常，这意味着根据视频流的编码格式（如 H.264、HEVC 等）找到对应的硬件加速解码器。

cpp
Copy code
// 假设你已经解析了视频流并确定了编码格式，例如 AV_CODEC_ID_H264
const AVCodec* decoder = avcodec_find_decoder_by_name("h264_cuvid"); // 对于 NVIDIA GPU，使用 NVDEC
if (!decoder) {
    decoder = avcodec_find_decoder(AV_CODEC_ID_H264); // 备用：软件解码器
    if (!decoder) {
        // 错误处理: 找不到解码器
    }
}

AVCodecContext* decoderCtx = avcodec_alloc_context3(decoder);
if (!decoderCtx) {
    // 错误处理: 无法分配解码器上下文
}
步骤 2: 关联硬件设备上下文
一旦你有了解码器上下文，你需要将其与你之前创建的硬件设备上下文相关联。这样 FFmpeg 在解码时就会使用指定的硬件设备。

cpp
Copy code
decoderCtx->hw_device_ctx = av_buffer_ref(hwDeviceCtx); // 引用硬件设备上下文
步骤 3: 打开解码器上下文
现在你可以尝试打开解码器上下文了。如果解码器支持硬件加速并且一切配置正确，这将启用硬件加速解码。

cpp
Copy code
if (avcodec_open2(decoderCtx, decoder, NULL) < 0) {
    // 错误处理: 无法打开解码器
}
步骤 4: 解码视频帧
你现在可以开始解码视频帧了。这通常涉及到读取视频流，将其送入解码器，并获取解码后的帧。这里只是一个高层次的示例，具体实现会涉及到处理视频流的细节。


encoder实现思路:
cpp
Copy code
AVPacket packet;
av_init_packet(&packet);
// 假设你已经填充了 packet

// 发送数据包到解码器
if (avcodec_send_packet(decoderCtx, &packet) < 0) {
    // 错误处理: 发送数据包失败
}

// 接收解码的帧
AVFrame* frame = av_frame_alloc();
if (avcodec_receive_frame(decoderCtx, frame) == 0) {
    // 成功获取到一帧
    // 处理解码后的帧，例如渲染或进一步处理
}

// 释放资源
av_frame_free(&frame);
av_packet_unref(&packet);
清理资源
在完成所有解码操作后，不要忘记清理你分配的所有资源，包括解码器上下文、硬件设备上下文以及任何其他你可能创建的 AVFrame 或 AVPacket 实例。

cpp
Copy code
avcodec_free_context(&decoderCtx);
av_buffer_unref(&hwDeviceCtx); // 注意: 如果你在其他地方还有引用，确保处理正确
请注意，这个示例非常简化，为了完整地实现硬件加速视频解码，你需要处理更多细节，如循环处理视频流中的所有数据包，以及正确地管理视频流的打开和关闭。同时，确保检查每个函数调用的返回值，以便正确处理错误情况。